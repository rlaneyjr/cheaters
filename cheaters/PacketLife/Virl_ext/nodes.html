
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>5. VIRL Nodes &#8212; VIRL 0.10.35.35 documentation</title>
    <link rel="stylesheet" href="_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.10.35.35',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="6. Configuration" href="configuration.html" />
    <link rel="prev" title="4. Projects, Users and Special Networks" href="concepts.html" /> 
  </head>
  <body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="configuration.html" title="6. Configuration"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="concepts.html" title="4. Projects, Users and Special Networks"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">VIRL 0.10.35.35 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="README.html" accesskey="U">README</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="virl-nodes">
<h1>5. VIRL Nodes<a class="headerlink" href="#virl-nodes" title="Permalink to this headline">¶</a></h1>
<div class="section" id="supported-simple-node-subtypes">
<h2>5.1. Supported SIMPLE node subtypes<a class="headerlink" href="#supported-simple-node-subtypes" title="Permalink to this headline">¶</a></h2>
<p>Each node subtype in a topology must be known to VIRL-CORE. The differences
between subtypes are encapsulated in VIRL plugins bundled with the package.</p>
<p>You can list the recognized subtypes in the UWM ‘Subtypes’ tab.</p>
<div class="section" id="openstack-vm-node-subtypes">
<h3>5.1.1. OpenStack VM node subtypes<a class="headerlink" href="#openstack-vm-node-subtypes" title="Permalink to this headline">¶</a></h3>
<p>The primary way of launching nodes is in the form of KVM instances managed
by OpenStack. These include all Cisco router images.</p>
<p>Note that in order to properly start a Cisco router image, it needs to have
properties attached, namely ‘hw_disk_bus’, ‘hw_vif_model’, ‘serial’ and
‘config_disk_type’.</p>
<p>The ‘hw_cdrom_bus’ image property may be used for VMs that use a different bus
on the main drive than on a secondary drive. CSR1000v should use ‘ide’ here.</p>
<p>These properties are essentially constant, with variations, if any, reserved
to internal development builds of the VMs. A different value than what the VM
operating system expects will generally result in a boot failure.</p>
<p>The VM disk images for all subtypes need not be available to all customers.</p>
<p>Supported subtypes, their interface names and recommended parameters include:</p>
<table border="1" class="docutils">
<colgroup>
<col width="12%" />
<col width="32%" />
<col width="32%" />
<col width="12%" />
<col width="13%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Subtype</th>
<th class="head">First interface</th>
<th class="head">Management interface</th>
<th class="head">Ram MB</th>
<th class="head">VCPUs</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>IOSv</td>
<td>GigabitEthernet0/1</td>
<td>GigabitEthernet0/0</td>
<td>512</td>
<td>1</td>
</tr>
<tr class="row-odd"><td>IOSvL2</td>
<td>GigabitEthernet0/1</td>
<td>Vlan1 (over GE0/0)</td>
<td>768</td>
<td>1</td>
</tr>
<tr class="row-even"><td>IOS XRv</td>
<td>GigabitEthernet0/0/0/0</td>
<td>MgmtEth0/0/CPU0/0</td>
<td>3072</td>
<td>1</td>
</tr>
<tr class="row-odd"><td>NX-OSv</td>
<td>Ethernet2/1</td>
<td>mgmt0</td>
<td>2048</td>
<td>1</td>
</tr>
<tr class="row-even"><td>CSR1000v</td>
<td>GigabitEthernet2</td>
<td>GigabitEthernet1</td>
<td>3072</td>
<td>1</td>
</tr>
<tr class="row-odd"><td>server</td>
<td>eth1</td>
<td>eth0</td>
<td>2048</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>The default properties that must be defined in VM disk images are as follows:</p>
<table border="1" class="docutils">
<colgroup>
<col width="12%" />
<col width="29%" />
<col width="22%" />
<col width="23%" />
<col width="14%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Subtype</th>
<th class="head">config_disk_type</th>
<th class="head">hw_disk_bus</th>
<th class="head">hw_vif_model</th>
<th class="head">serial</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>IOSv</td>
<td>disk</td>
<td>virtio</td>
<td>e1000</td>
<td>2</td>
</tr>
<tr class="row-odd"><td>IOSvL2</td>
<td>disk</td>
<td>virtio</td>
<td>e1000</td>
<td>2</td>
</tr>
<tr class="row-even"><td>IOS XRv</td>
<td>cdrom</td>
<td>ide</td>
<td>virtio</td>
<td>3</td>
</tr>
<tr class="row-odd"><td>NX-OSv</td>
<td>cdrom</td>
<td>ide</td>
<td>e1000</td>
<td>2</td>
</tr>
<tr class="row-even"><td>CSR1000v</td>
<td>cdrom</td>
<td>virtio</td>
<td>e1000</td>
<td>2</td>
</tr>
<tr class="row-odd"><td>server</td>
<td>cloud-init, iso9660</td>
<td>virtio</td>
<td>virtio</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>The server subtype is mainly geared towards the bundled Ubuntu cloud image.
A Fedora cloud image can also be successfully deployed using the subtype.</p>
</div>
<div class="section" id="linux-container-subtypes">
<h3>5.1.2. Linux Container subtypes<a class="headerlink" href="#linux-container-subtypes" title="Permalink to this headline">¶</a></h3>
<p>Users can extend and connect their simulations with Linux Containers. These
are meant to be a lightweight yet powerful alternative to servers.</p>
<p>VIRL has out-of the box support for 3 subtypes of LXC’s.</p>
<p>The first, lxc, is configured with a template that expects a slightly
modified Ubuntu LXC image (downloadable via UWM VIRL Software pages).
This subtype is very similar in operation to the Ubuntu server OpenStack VM,
including being configured by the same cloud-init structures as that subtype.</p>
<p>Another is the management LXC automatically created for the simulation, which
is controlled by extensions on the topology element. This subtype is not meant
to be included by users in topologies.</p>
<p>The final subtype lxc-iperf, serves as an example of a thin layer above the
host server system. Unlike the Ubuntu LXC, and in common with the management
LXC, it does not contain a full distribution of packages, opting instead to
attach (bind-mount) itself to its host’s filesystems in read-only mode.
The iperf image contains only the built iperf (version 2) binary.</p>
<p>Cloud-init is not available to this system, but extensions defined below can
be used control its setup. For more information on iperf, its features
and usage, please refer to <a class="reference external" href="http://iperf.fr">http://iperf.fr</a>.</p>
<p>LXC images, unlike OpenStack VMs, are simply tar.gz archives of files that
comprise (part of) the node’s filesystem. The management and setup of each
LXC subtype is governed by its template, a (bash) script executed (twice)
for each LXC node being first created, then started.</p>
<p>The UWM system allows admins to install their own LXC template scripts,
allowing for creation of specialized nodes running custom applications.
In a given subtype, the baseline_flavor attribute is used to name the default
template to run the LXC. This can be overridden by each node’s flavor.</p>
<p>Both the management LXC and lxc-iperf scripts shipped with VIRL are customized
templates for the SSH server, hence SSH is running in each of them and
users can log into them (using the username/password of the user in UWM/STD
in the case of management LXC, or cisco/cisco in lxc-iperf).</p>
<p>Note that sudo is not available to the LXC user, hence any setup requiring
the root (limited in the LXC) account needs to be executed in the “start”
mode execution of the template.</p>
<p>The lxc-iperf template is recommended as an example of adding one’s own LXC’s
into the VIRL system. An LXC strictly does not need any image at all, if
all required packages are installed on the host system. Since the usual
locations where executable files are located on a linux system are read-only
inside the LXC, the image needs to be extracted in a different location, e.g.
/lxc. The environment variables PATH and LD_LIBRARY_PATH should be adjusted
by the template to make searchable the added executables and shared libraries,
respectively. The actual image to use may be an empty dummy tarball.</p>
<p>The interface setup of the LXC should be left intact for configuration by
the STD according to the network links of the topology. The network interfaces
of the LXCs are created in a similar fashion to the interfaces of OpenStack
VMs, making the networking experience rather seamless. Note that link-state
changes are not performed on LXC interfaces.</p>
</div>
<div class="section" id="dynamic-subtype-plugins">
<h3>5.1.3. Dynamic subtype plugins<a class="headerlink" href="#dynamic-subtype-plugins" title="Permalink to this headline">¶</a></h3>
<p>Some subtypes’ behavior may conform entirely to the behavior of an existing
subtype. A “dynamic” subtype may be defined with these different values.
The plugin lives as a section in a configuration file named node_subtype.cfg,
whose location is determined by VIRL package configuration. The ini-style
configuration section name gives the name of the plugin.</p>
<p>Note that the runtime behavior, e.g. configuration retrieval from a running VM
node, cannot be adjusted this way.</p>
<p>The values that can be adjusted are as follows:</p>
<dl class="docutils">
<dt>interface_management</dt>
<dd>Name of management interface, the first interface of each node. Empty value
signifies that a management interface is not to be defined for the node;
the management network will not be connected to such a node.</dd>
<dt>interface_dummies</dt>
<dd>Whitespace-separated sequence of interface names to statically inject between
the management interface and the first data interface. These interfaces will
each reside on a separate network, not being able to send traffic anywhere.
This may be used for VMs which expose additional management interfaces used
by the users of the nodes.</dd>
<dt>interface_pattern</dt>
<dd>Pattern for data interface names. A string with a single “{0}” placeholder
where the interface number shall be placed. An optional “{1}” placeholder may
be used if the interface_wrap is nonzero.</dd>
<dt>interface_first</dt>
<dd>First data interface number. This number is added to the id of each interface
to produce the interface name from the interface_pattern.</dd>
<dt>interface_range</dt>
<dd>Max count of data interfaces. KVM places a hard limit on interfaces to 28.
The management interface is counted towards this limit.</dd>
<dt>interface_wrap</dt>
<dd>If not zero, the interface number is reset to zero after this number of ports
is reached. A second counter is incremented, which starts at 0 and uses the
optional {1} marker in the interface_pattern.</dd>
<dt>cli_serial</dt>
<dd>Number of serial console interfaces to be defined for the node.
This value is used by UWM to be placed as a VM image property for images
of that subtype. It also determines the allowed range of websocket-proxied
serial ports for STD.</dd>
<dt>cli_protocol</dt>
<dd>Protocol used to connect to the node from the management proxy (jumphost or
lxc node). Valid values are “none”, “ssh” and “telnet”.
Note that serial console interfaces use telnet and are not affected by this.</dd>
<dt>vnc_available</dt>
<dd>Make VNC access available. This flag indicates whether the VNC console, which
is actually created for each VM node, is actually usable. Most Cisco routers
cannot make use of the VNC console, leaving remote control to serial ports.</dd>
<dt>gui_icon</dt>
<dd>Name of icon for GUI. See the subtype configuration dialog of the GUI.</dd>
<dt>gui_visible</dt>
<dd>Show subtype on GUI palette in the topology design view. Invisible subtypes
may still be selected in the GUI with the subtype node property.</dd>
<dt>config_disk_type</dt>
<dd>Configuration disk type. Placed into VM image properties for consumption by
OpenStack when a node is deployed. Also controls the format of configuration
data sent to OpenStack with the node deployment request.</dd>
<dt>config_iso_level</dt>
<dd>ISO 9660 level for cdrom config_disk_type</dd>
<dt>config_file</dt>
<dd>Name of file for config drive. The processed “config” extension entry content
is placed into the file of this name on the configuration drive. A special
“/userdata” channel may be used, having meaning in “cloud-init” config drive.</dd>
<dt>hw_vif_model</dt>
<dd>Virtual interface model to simulate. Placed into VM image properties by UWM.</dd>
<dt>hw_disk_bus</dt>
<dd>Main disk bus model to simulate. Placed into VM image properties by UWM.</dd>
<dt>hw_ram</dt>
<dd>RAM (MB) allocated per node. Placed into the default subtype VM flavor.</dd>
<dt>hw_vcpus</dt>
<dd>Number of CPUs allocated per node. Placed into the default subtype VM flavor.</dd>
<dt>hw_vm_extra</dt>
<dd>Extra comma-separated VM image properties to be added by UWM.</dd>
<dt>baseline_image</dt>
<dd>Name of default image. Default (empty) means the subtype name shall be used.</dd>
<dt>baseline_flavor</dt>
<dd>Name of default flavor. Default (empty) means the subtype name shall be used.
For LXC’s, this is actually the name of the template to run.</dd>
</dl>
</div>
</div>
<div class="section" id="images-and-flavors">
<h2>5.2. Images and flavors<a class="headerlink" href="#images-and-flavors" title="Permalink to this headline">¶</a></h2>
<p>The VM disk image contains the bootable operating system disk for the VM.
A flavor is a collection of resources made available to each running node
by the OpenStack orchestration. The recommended (expected) flavor attributes
are described in the previous section. All other attributes should be set to 0.</p>
<p>Each node may select its own image and flavor by name as node attributes.
By default, the node subtype is used for both attributes. The lookup of the
concrete image or flavor first matches the item prefixed with user’s project
name (e.g. myproject-IOSv), before trying the plain name. For images, another
preference is set for images owned by the user’s project over any other images,
as OpenStack allows to name multiple images the same. In case of a tie, newest
images will be picked.</p>
<p>As noted in the previous section, all VM disk images for a Cisco router must
have their properties set up, strongly preferred to the values listed there.</p>
<p>Image names are case sensitive, meaning an image will only be selected if its
name matches the value specified for the node exactly. On the other hand,
flavor names are case insensitive in OpenStack, hence any spelling will work
for flavors. It is nonetheless recommended to match character case of flavors.</p>
<p>Please note that LXC images are actually tar.gz archives, not complete file
systems, and LXC flavors are actually names of templates to execute when the
node is being created and later started.</p>
</div>
<div class="section" id="initial-node-configuration">
<h2>5.3. Initial node configuration<a class="headerlink" href="#initial-node-configuration" title="Permalink to this headline">¶</a></h2>
<p>Each VM node can be started with a second disk or cdrom drive containing the
contents of the “config” node extension entry formatted for that node subtype.
The nodes’ operating system reads that information and applies it to its
configuration.</p>
<p>In fact, some VM images, such as the Ubuntu Cloud image, may be completely
unusable without such configuration.</p>
<p>There is a hard limit on the size of such configurations. VIRL will compress
the configuration data when transferring it into OpenStack; the compressed size
of all configuration data (gzip) shall not exceed ~60KB. This limitation is set
by the storage implementation of this data within OpenStack.</p>
<p>Some string constants will be substituted for all VM node subtypes with values
determined at runtime (all of which are most likely useful for server nodes):</p>
<dl class="docutils">
<dt>VIRL-USER-NAME</dt>
<dd>The OpenStack username (usually the same as the STD username)</dd>
<dt>VIRL-USER-PASSWORD-CRYPT</dt>
<dd><p class="first">An SHA512 hashed and salter OpenStack password of the user.</p>
<p>Note that UWM-created users have OpenStack passwords different from their STD
passwords, which can be determined by using this command:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">VIRL_STD_PASSWORD</span><span class="o">=</span><span class="s2">&quot;password&quot;</span> <span class="n">virl_uwm_client</span> <span class="n">make</span><span class="o">-</span><span class="n">os</span><span class="o">-</span><span class="n">password</span>
</pre></div>
</div>
<p class="last">One should not leave password command invocations in the shell history.</p>
</dd>
<dt>VIRL-USER-SSH-PUBLIC-KEY</dt>
<dd>An SSH public key fingerprint, configurable in UWM for each user, which may
be placed into the configuration in order to accept the related private key
in ssh/sftp/scp operations.</dd>
<dt>VIRL-SIMULATION-NAME</dt>
<dd>The full string name of the simulation the node is running within.</dd>
</dl>
<div class="section" id="cisco-router-configuration">
<h3>5.3.1. Cisco router configuration<a class="headerlink" href="#cisco-router-configuration" title="Permalink to this headline">¶</a></h3>
<p>Initial configuration for Cisco router VMs may be defined in the VIRL file as
text in the same format as output by the ‘show startup-config’ command for that
VM subtype.</p>
<p>The first, management interface, as well as all interfaces connected to L2 and
L3 connectors to the outside environment, have their IP addresses assigned
for the duration of the node’s run by the orchestration system. The address
lines in the “config” for such interfaces are detected and replaced
with the assigned value when the node starts. It is recommended to leave the
address line as ‘no ip(v4) address’ in the configuration text. If the address
statement is not present, no substitution takes place.</p>
<p>Some control of this IP address injection process can be achieved by putting
a “REWRITE_IP: interface_name” directive at the end of a “description” line
before the ip address statement that is to be affected by the directive.
One ip address line following the directive will be processed as if it belonged
to that interface.</p>
<p>Hence, the following configuration can suppress the injection for an interface,
forcing an arbitrary static IP address in the configuration:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>interface GigabitEthernet0/1
  description to flat-2 REWRITE_IP: none
  no shutdown
  ip address 1.2.3.4 255.255.255.0
!
</pre></div>
</div>
<p>The following configuration can achieve IP address injection onto a vlan
subinterface instead of the main L2 FLAT-connected interface. Note that each
interface name has only one chance of being rewritten, hence the reversed
order of the interfaces in the configuration (that will be lost with each
configuration extraction):</p>
<div class="highlight-default"><div class="highlight"><pre><span></span>interface GigabitEthernet0/2.111
  description to flat-1 REWRITE_IP: GigabitEthernet0/2
  encapsulation dot1Q 111
  no ip address
  duplex auto
  speed auto
  no shutdown
!
interface GigabitEthernet0/2
  no ip address
  duplex auto
  speed auto
  no shutdown
!
</pre></div>
</div>
<p>To achieve the same effect without REWRITE_IP, another mechanism may be applied
by adding an extension entry with key “vlan” to the L2 FLAT node with the value
set to “111”, or equivalently by setting a “subinterface” extension entry on L2
FLAT with value “.111”. If the value of “subinterface” is set to any value that
produces an interface name not present in the source “config” entry, no match
for the interface name will be found, and no substitution will be performed.</p>
<p>In addition to the router config, any (textual) data contained in extension
entries with keys “initial config: filename” will be added to the configuration
disk for the VM under that same filename (subdirectories are not supported).</p>
</div>
<div class="section" id="cloud-init-server-configuration">
<h3>5.3.2. Cloud-init server configuration<a class="headerlink" href="#cloud-init-server-configuration" title="Permalink to this headline">¶</a></h3>
<p>Unlike router nodes, server VMs may have very different configuration styles.
One such style is iso9660, which creates a simulated cdrom from all initial
config files (subdirectories are supported). An enhanced approach is using the
cloud-init software package on the guest VM, and creation of a cloud-init cdrom
image from the initial config files.</p>
<p>With cloud-init, the filename from the initial config entry key becomes target
path for the file in question, copied from the cdrom by the cloud-init program.
All such files will be owned by the root user, unreadable to any other users.</p>
<p>In the server subtype, the “config” extension entry is mapped to a “/userdata”
filename as that is the location where OpenStack expects the cloud-init control
definitions in a YAML-formatted dictionary. The only processing of this
data by the LLI is to replace the string occurrences as described in the common
section. No Outside address injection into config takes place;
the server/non-router VM nodes should employ DHCP to configure such interfaces.</p>
<p>AutoNetkit will generate a basic cloud-init file; for some further examples
refer to <cite>http://cloudinit.readthedocs.org/en/latest/topics/examples.html</cite>.
The generated configuration is usable without change by both Ubuntu 13.10+ and
Fedora 20 cloud-init images.</p>
<p>Please note that Ubuntu-cloud distributions prior to the 13.10 release may
fail to process all the configuration and become unusable as a result.</p>
<p>Cloud-init is also available for the main lxc subtype using the ubuntu-ci image
distributed alongside other user images.</p>
</div>
<div class="section" id="the-jumphost-session">
<h3>5.3.3. The Jumphost session<a class="headerlink" href="#the-jumphost-session" title="Permalink to this headline">¶</a></h3>
<p>The STD server will create and maintain a special session named ‘~jumphost’,
which contains a single pre-configured node named ‘jumphost’, with a connection
to the default L2 FLAT network. It uses bound ports named ‘username-jumphost’,
which will be created with random available IP addresses if they don’t exist.</p>
<p>This simulation cannot be stopped (destroyed) by normal process; on the other
hand, stopping its node will free up all resources it holds in OpenStack.
An administrator user can stop this simulation via UWM and an STD call.</p>
<p>The jumphost node contains the same user as the username, and accepts the same
password as the STD password of that user. It also accepts the same SSH public
key as normal servers. After a password/key change, restarting the jumphost
node (which does not happen automatically) propagates this change.</p>
<p>The jumphost node provides access to simulation nodes using the private project
management network or an L2 FLAT management network; it cannot reach any
per-simulation management network.</p>
<p>Simulations employing the new LXC-based management network do not require the
jumphost session, hence it is not created and maintained for such sessions, and
the node is torn down despite there being LXC-based sessions deployed.</p>
<p>Similar to the LXC SSH port forwarding, the jumphost node’s SSH port is also
exposed from the VIRL host. The port number can be controlled in the UWM user
settings dialogs.</p>
</div>
<div class="section" id="lxc-iperf-node-subtype-configuration">
<h3>5.3.4. LXC Iperf node subtype configuration<a class="headerlink" href="#lxc-iperf-node-subtype-configuration" title="Permalink to this headline">¶</a></h3>
<p>The lxc-perf subtype cannot process cloud-init configuration. It ignores the
“config” node extension completely, using the following extensions instead:</p>
<dl class="docutils">
<dt>custom_script</dt>
<dd><p class="first">The value is a bash script which gets executed by a (simulated) root user.
By default, VIRL will attempt to generate a script assigning a default
route to this system by using an address of a directly connected peer node.
Override this script to force a more suitable gateway or other configuration
within the system. To set a default route, enter:</p>
<div class="last highlight-default"><div class="highlight"><pre><span></span><span class="n">route</span> <span class="n">add</span> <span class="n">default</span> <span class="n">gw</span> <span class="n">A</span><span class="o">.</span><span class="n">B</span><span class="o">.</span><span class="n">C</span><span class="o">.</span><span class="n">D</span>
</pre></div>
</div>
</dd>
<dt>iperf_server</dt>
<dd><p class="first">The value of this extension is a JSON Object (mapping, dict). Each value as
well as the entire extension are optional. An iperf server is always started
in the node, awaiting client connections. A log is maintained for the server,
which can be retrieved by opening a TCP connection on port 5555 of the node.</p>
<p>The valid keys for the extension are as follows. Each key represents a switch
on the command line. Switches marked (F) are applied if the key is false.</p>
<table border="1" class="last docutils">
<colgroup>
<col width="24%" />
<col width="10%" />
<col width="8%" />
<col width="58%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Key and Type</th>
<th class="head">Default</th>
<th class="head">Switch</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>server_port int</td>
<td>5001</td>
<td>-p</td>
<td>TCP/UDP port for iperf client connections</td>
</tr>
<tr class="row-odd"><td>tcp bool</td>
<td>true</td>
<td>-u (F)</td>
<td>Select whether TCP or UDP traffic is used</td>
</tr>
<tr class="row-even"><td>refresh_interval</td>
<td>1</td>
<td>-i</td>
<td>Refresh interval, 0 for end summary only</td>
</tr>
<tr class="row-odd"><td>window_size str</td>
<td>“80K”</td>
<td>-w</td>
<td>Define size of the window</td>
</tr>
<tr class="row-even"><td>length str</td>
<td>“8K”</td>
<td>-l</td>
<td>The length of buffers to read or write</td>
</tr>
</tbody>
</table>
</dd>
<dt>iperf_client</dt>
<dd><p class="first">The value of this extension is a JSON Object (mapping, dict). If the value
is present, a process listening on port 6666 of the node is created, which,
when retrieved (e.g. by curl node_ip_address:5555) runs a preconfigured iperf
client, returning the test results on the client side.</p>
<p>The valid keys for the extension are as follows. Each key represents a switch
on the command line. Switches marked (F) are applied if the key is false.</p>
<table border="1" class="last docutils">
<colgroup>
<col width="24%" />
<col width="10%" />
<col width="8%" />
<col width="58%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Key and Type</th>
<th class="head">Default</th>
<th class="head">Switch</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>server_ip str</td>
<td>none</td>
<td>-c</td>
<td>IP address of an iperf server</td>
</tr>
<tr class="row-odd"><td>server_port int</td>
<td>5001</td>
<td>-p</td>
<td>Port of the listening iperf server</td>
</tr>
<tr class="row-even"><td>client_port int</td>
<td>5002</td>
<td>-L</td>
<td>TCP port if dual_test is true, see below</td>
</tr>
<tr class="row-odd"><td>tcp bool</td>
<td>true</td>
<td>-u (F)</td>
<td>Select whether TCP or UDP traffic is used</td>
</tr>
<tr class="row-even"><td>udp_bandwidth</td>
<td>“1M”</td>
<td>-b</td>
<td>Define the speed of UDP connection</td>
</tr>
<tr class="row-odd"><td>refresh_interval</td>
<td>1</td>
<td>-i</td>
<td>Refresh interval, 0 for end summary only</td>
</tr>
<tr class="row-even"><td>test_duration int</td>
<td>5</td>
<td>-t</td>
<td>Second to run the test</td>
</tr>
<tr class="row-odd"><td>window_size str</td>
<td>“80K”</td>
<td>-w</td>
<td>Define size of the window</td>
</tr>
<tr class="row-even"><td>length str</td>
<td>“8K”</td>
<td>-l</td>
<td>The length of buffers to read or write</td>
</tr>
<tr class="row-odd"><td>connections int</td>
<td>1</td>
<td>-p</td>
<td>Number of connections created</td>
</tr>
<tr class="row-even"><td>dual_test bool</td>
<td>false</td>
<td>-d</td>
<td>Traffic will be tested in both directions</td>
</tr>
<tr class="row-odd"><td>simultaneously</td>
<td>true</td>
<td>-r (F)</td>
<td>Make dual_test connections simultaneously</td>
</tr>
<tr class="row-even"><td>ttl int</td>
<td>255</td>
<td>-T</td>
<td>Max number of router hops to go through</td>
</tr>
</tbody>
</table>
</dd>
</dl>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">5. VIRL Nodes</a><ul>
<li><a class="reference internal" href="#supported-simple-node-subtypes">5.1. Supported SIMPLE node subtypes</a><ul>
<li><a class="reference internal" href="#openstack-vm-node-subtypes">5.1.1. OpenStack VM node subtypes</a></li>
<li><a class="reference internal" href="#linux-container-subtypes">5.1.2. Linux Container subtypes</a></li>
<li><a class="reference internal" href="#dynamic-subtype-plugins">5.1.3. Dynamic subtype plugins</a></li>
</ul>
</li>
<li><a class="reference internal" href="#images-and-flavors">5.2. Images and flavors</a></li>
<li><a class="reference internal" href="#initial-node-configuration">5.3. Initial node configuration</a><ul>
<li><a class="reference internal" href="#cisco-router-configuration">5.3.1. Cisco router configuration</a></li>
<li><a class="reference internal" href="#cloud-init-server-configuration">5.3.2. Cloud-init server configuration</a></li>
<li><a class="reference internal" href="#the-jumphost-session">5.3.3. The Jumphost session</a></li>
<li><a class="reference internal" href="#lxc-iperf-node-subtype-configuration">5.3.4. LXC Iperf node subtype configuration</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="concepts.html"
                        title="previous chapter">4. Projects, Users and Special Networks</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="configuration.html"
                        title="next chapter">6. Configuration</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/nodes.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="configuration.html" title="6. Configuration"
             >next</a> |</li>
        <li class="right" >
          <a href="concepts.html" title="4. Projects, Users and Special Networks"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">VIRL 0.10.35.35 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="README.html" >README</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2016, Cisco Systems, Inc.
All rights reserved..
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.6.7.
    </div>
  </body>
</html>